{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ありがとうございました!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from flask import Flask, render_template, request, make_response \n",
    "\n",
    "# 環境変数を読み込む\n",
    "load_dotenv(\".env\")\n",
    "os.environ['OPENAI_API_KEY'] = os.environ['API_KEY']\n",
    "\n",
    "# 使用するモデル名\n",
    "MODEL_NAME = \"gpt-4o-mini\" \n",
    "\n",
    "# MemorySaverインスタンスの作成\n",
    "memory = MemorySaver()\n",
    "\n",
    "# グラフを保持する変数の初期化\n",
    "graph = None\n",
    "\n",
    "# ===== Stateクラスの定義 =====\n",
    "# Stateクラス: メッセージのリストを保持する辞書型\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# ===== グラフの構築 =====\n",
    "def build_graph(model_name, memory):\n",
    "    \"\"\"\n",
    "    グラフのインスタンスを作成し、ツールノードやチャットボットノードを追加します。\n",
    "    モデル名とメモリを使用して、実行可能なグラフを作成します。\n",
    "    \"\"\"\n",
    "    # グラフのインスタンスを作成\n",
    "    graph_builder = StateGraph(State)\n",
    "\n",
    "    # ツールノードを作成（TavilySearchResultsを使用）\n",
    "    tavily_tool = TavilySearchResults(max_results=2)\n",
    "    tools = [tavily_tool]\n",
    "    tool_node = ToolNode(tools)\n",
    "    graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "    # チャットボットノードの作成\n",
    "    llm = ChatOpenAI(model_name=model_name)\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    \n",
    "    # チャットボットの実行方法を定義\n",
    "    def chatbot(state: State):\n",
    "        return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "    \n",
    "    graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "    # 実行可能なグラフの作成\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"chatbot\",\n",
    "        tools_condition,\n",
    "    )\n",
    "    graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "    graph_builder.set_entry_point(\"chatbot\")\n",
    "    \n",
    "    return graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# ===== グラフを実行する関数 =====\n",
    "def stream_graph_updates(graph: StateGraph, role: str, user_message: str):\n",
    "    \"\"\"\n",
    "    ユーザーからのメッセージを元に、グラフを実行し、チャットボットの応答をストリーミングします。\n",
    "    \"\"\"\n",
    "    response = graph.invoke(\n",
    "        {\"messages\": [(\"user\", user_message), (\"system\", role)]},\n",
    "        {\"configurable\": {\"thread_id\": \"1\"}},\n",
    "        stream_mode=\"values\"\n",
    "    )\n",
    "    return response[\"messages\"][-1].content\n",
    "\n",
    "# ===== 応答を返す関数 =====\n",
    "def get_bot_response(user_message, role, memory):\n",
    "    \"\"\"\n",
    "    ユーザーのメッセージに基づき、ボットの応答を取得します。\n",
    "    初回の場合、新しいグラフを作成します。\n",
    "    \"\"\"\n",
    "    global graph\n",
    "    # グラフがまだ作成されていない場合、新しいグラフを作成\n",
    "    if graph is None:\n",
    "        graph = build_graph(MODEL_NAME, memory)\n",
    "\n",
    "    # グラフを実行してボットの応答を取得\n",
    "    return stream_graph_updates(graph, role, user_message)\n",
    "\n",
    "# ===== メッセージの一覧を取得する関数 =====\n",
    "def get_messages_list(memory):\n",
    "    \"\"\"\n",
    "    メモリからメッセージ一覧を取得し、ユーザーとボットのメッセージを分類します。\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    # メモリからメッセージを取得\n",
    "    memories = memory.get({\"configurable\": {\"thread_id\": \"1\"}})['channel_values']['messages']\n",
    "    for message in memories:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            # ユーザーからのメッセージ\n",
    "            messages.append({'class': 'user-message', 'text': message.content})\n",
    "        elif isinstance(message, AIMessage) and message.content != \"\":\n",
    "            # ボットからのメッセージ（最終回答）\n",
    "            messages.append({'class': 'bot-message', 'text': message.content})\n",
    "    return messages[-1]['text']\n",
    "\n",
    "role = input(\"キャラクター設定:\")\n",
    "while True:\n",
    "    user_input = input(\"質問:\")\n",
    "    if user_input.strip()==\"\":\n",
    "        print(\"ありがとうございました!\")\n",
    "        break\n",
    "    \n",
    "    get_bot_response(user_input, role, memory)\n",
    "    print(get_messages_list(memory))\n",
    "# ソースコードを記述"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
